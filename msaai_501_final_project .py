# -*- coding: utf-8 -*-
"""MSAAI_501 Final Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15_pyX_zWnkMVYfMgSdWq5JTvRZd3910H
"""

#All imports for the assignment

# Data manipulation and statistics
import pandas as pd
import numpy as np
from scipy import stats
from tabulate import tabulate

# Visualization
import seaborn as sns
import matplotlib.pyplot as plt
import IPython.display as disp

# Preprocessing and model selection
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Classification models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

# Evaluation metrics
from sklearn.metrics import (accuracy_score, precision_score, recall_score,f1_score, roc_auc_score, classification_report, confusion_matrix)

# Deep learning
import tensorflow as tf
from tensorflow.keras import layers, models

# Explainability
import shap

# Read in the dataset.
df = pd.read_csv("/content/breast-cancer.csv")

# Print Head
df.head()

# Print Head
df.head()

# Check the dataframe for zero and nan values

df.info()

# Check for null values
df.isnull().sum()

# Check for zero values
(df == 0).sum()

# Isolate only the columns which contain zero values

zero_counts = (df == 0).sum()
zero_columns = zero_counts[zero_counts > 0]
print(zero_columns)

##Remove the ID column since it doesn't help with analysis
if "id" in df.columns:
    df = df.drop(columns=["id"])

# Select numeric columns and not catagorical, I don't think there are any but just in case
columns = df.select_dtypes(include="number")

# Impute missing values with the median for each numeric column
for col in columns.columns:
    median_val = df[col].median()
    df[col] = df[col].fillna(median_val)

# Calculate descriptive stats
BC_descriptive = {}

for col in columns.columns:
    col_data = df[col]

    # Descriptive stats for dictionary
    mode_values = col_data.mode()
    if not mode_values.empty:
        mode_result = str(round(mode_values.iloc[0], 2))
        if len(mode_values) > 1:
            mode_result += "..."
    else:
        mode_result = None

    BC_descriptive[col] = {
        "mean": round(col_data.mean(), 2),
        "median": round(col_data.median(), 2),
        "mode": mode_result,
        "std_dev": round(col_data.std(), 2),
        "num_unique_values": int(col_data.nunique())
    }

# Convert to a dataframe so it's easy to read
rounded_stats_df = pd.DataFrame(BC_descriptive).T

# Print
print("Descriptive Statistics Table:\n")
print(tabulate(rounded_stats_df, headers="keys", tablefmt="grid"))

# map diagnosis
df["diagnosis_full"] = df["diagnosis"].replace({0: "Benign", 1: "Malignant", "B": "Benign", "M": "Malignant"})

# count labels
counts = df["diagnosis_full"].value_counts()

# make everything pink for breast cancer
color_map = {"Benign": "lightpink", "Malignant": "deeppink"}

bar_colors = []
for label in counts.index:
    if label in color_map:
        bar_colors.append(color_map[label])

# plot
plt.figure(figsize=(4, 4))
counts.plot(kind="bar", color=bar_colors)
plt.title("Number of Benign vs Malignant Tumors")
plt.xlabel("Tumor Type")
plt.ylabel("Number of Cases")
plt.xticks(rotation=0)
plt.grid(axis="y")
plt.tight_layout()
plt.show()

# Select only numeric columns
numeric_cols = df.select_dtypes(include=np.number).columns.tolist()

# Set number of plots per page
# Batch the plots in groups of 9 for readability
# And Functionality in Matplotlib.

plots_per_page = 9
num_pages = int(np.ceil(len(numeric_cols) / plots_per_page))

for page in range(num_pages):
    start = page * plots_per_page
    end = start + plots_per_page
    cols_to_plot = numeric_cols[start:end]

    plt.figure(figsize=(18, 12))
    for i, col in enumerate(cols_to_plot, 1):
        plt.subplot(3, 3, i)
        sns.histplot(df[col], kde=True, bins=30, color='steelblue')
        plt.title(f"Distribution of {col}")
        plt.xlabel(col)
        plt.ylabel("Frequency")

    plt.tight_layout()
    plt.show()

  ## We likely don't need to include all histograms in the final report
  ## but we can mention that we visualized all of them and all were right skewed
  ## with the exception of texture (worst and mean) and smoothness (worst and mean)
  ## This is handled with the scaler

# Convert diagnosis to numeric
df["diagnosis"] = df["diagnosis"].map({"B": 0, "M": 1})

# Drop non-numeric columns (like IDs or text columns)
df_numeric = df.select_dtypes(include=["number"])

# Compute correlation matrix on numeric data only
corr_matrix = df_numeric.corr()

# Set figure size
plt.figure(figsize=(16, 12))

# Create heatmap
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, linewidths=0.5)

# Add title
plt.title("Feature Correlation Heatmap", fontsize=16)
plt.show()

# drop Diagnosis and ID, as they will have meaningless correlatiions

corr_with_target = corr_matrix['diagnosis'].drop(labels=['diagnosis', 'id'], errors='ignore')
corr_with_target = corr_with_target.sort_values(ascending=False)

print(corr_with_target)

# Regression analysis using Diagnosis as our target (y) variable

# set our x and y variables
## I used chatgpt to help debug the issue of having inconsistent numbers of samples
## as a result of dropping the ID and Diagnosis.  -LY
X = df.select_dtypes(include='number').drop(columns=['diagnosis', 'id'], errors='ignore')
y = df['diagnosis']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)


imputer = SimpleImputer(strategy='median')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# apply standard scaler to our X variables
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)

# Train logistic regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred = model.predict(X_test_scaled)

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

#Random Forest

#https://www.geeksforgeeks.org/random-forest-classifier-using-scikit-learn/

#initialize and train the model
rf_model = RandomForestClassifier(n_estimators=100, random_state=10)
rf_model.fit(X_train_scaled, y_train)

y_rf_pred = rf_model.predict(X_test_scaled)

print("Random Forest Confusion Matrix:")
print(confusion_matrix(y_test, y_rf_pred))

print("\nRandom Forest Classification Report:")
print(classification_report(y_test, y_rf_pred))

#Random forrest with some feature selection, just the first 15


#https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html
# extract top 15 features
importances = rf.feature_importances_
feature_names = X_train.columns
feat_imp_df = pd.DataFrame({"Feature": feature_names,"Importance": importances}).sort_values(by="Importance", ascending=False)

top_features = feat_imp_df["Feature"].head(15).values.tolist()
print("Top 15 features:", top_features)

# select and scale top features
X_train_top = X_train[top_features]
X_test_top = X_test[top_features]

scaler = StandardScaler()
X_train_top_scaled = scaler.fit_transform(X_train_top)
X_test_top_scaled = scaler.transform(X_test_top)

# retrain model on top 15 features
rf_top = RandomForestClassifier(n_estimators=100, random_state=42)
rf_top.fit(X_train_top_scaled, y_train)

# evaluate
y_pred_top = rf_top.predict(X_test_top_scaled)
y_probs_top = rf_top.predict_proba(X_test_top_scaled)[:, 1]


print("\nConfusion Matrix (Top 15 Features):")
print(confusion_matrix(y_test, y_pred_top))

print("\nClassification Report (Top 15 Features):")
print(classification_report(y_test, y_pred_top))

#plot the top 15 most important features
plt.figure(figsize=(8, 6))
sns.barplot(data=feat_imp_df.head(15), x="Importance", y="Feature", palette="viridis") #https://www.practicalpythonfordatascience.com/ap_seaborn_palette
plt.title("Top 15 Important Features (Random Forest)")
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

#Gradient Boosting
#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html

# initialize and train the model
gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42) # using .1 for a balanced approach
gb_model.fit(X_train_scaled, y_train)

# predict
y_gb_pred = gb_model.predict(X_test_scaled)

# evaluate
print("Gradient Boosting Confusion Matrix:")
print(confusion_matrix(y_test, y_gb_pred))

print("\nGradient Boosting Classification Report:")
print(classification_report(y_test, y_gb_pred))

## SVM Model
#https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html

# Initialize the model
svm_model = SVC(kernel="rbf", probability=True, random_state=42) #rbf is used for nonlinear
svm_model.fit(X_train_scaled, y_train)

# Predict on test set
y_svm_pred = svm_model.predict(X_test_scaled)

# Evaluate
print("SVM Confusion Matrix:")
print(confusion_matrix(y_test, y_svm_pred))

print("\nSVM Classification Report:")
print(classification_report(y_test, y_svm_pred))

# MLP model
#https://www.tensorflow.org/guide/core/mlp_core
#Used chatgpt to help improve model after initial made

# Changing y values to 0 and 1 and make arrays,
y_train_bin = y_train.values
y_test_bin = y_test.values

#####################################################################
# Define the structure of our neural network
#https://www.tensorflow.org/guide/keras/sequential_model
"""
neuron- takes the inputs and applys weights, to few underfitting to many overfitting
sigmoid function- makes all numbers between 0 and 1
ReLU- Rectified Linear unit, keeps only positive
Composed of input, hidden, and output layer
"""
mlp_model = models.Sequential([
    # First hidden layer, used 32 because that what was on tensorflow.org and worked well
    layers.Dense(32, activation="relu", input_shape=(X_train_scaled.shape[1],)),

    # Second hidden layer
    layers.Dense(32, activation="relu"),

    # Output layer with 1 neuron and used sigmoid which is what is used for binary classification
    layers.Dense(1, activation="sigmoid")])

#####################################################################

# put the model together
"""
Optimizer - Adam or adaptive moment estimation is an optimazation algorithm that adapts learning rate based on past
Loss function - binary_crossentropy is what is used here since it is binary, measures how wrong a prediciton is
Metric accuracy - used to tell how well a model is doing
"""
mlp_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0005), # learning rate determined by experimentation
    loss="binary_crossentropy",
    metrics=["accuracy"])

#################################################################
# train the model
"""
Epochs* how many times the model will go through the data
Batch size- how many samples the model sees at once
Validation split- use 20% of training data to validate the model during training
"""
history = mlp_model.fit(
    X_train_scaled,         # training features
    y_train_bin,            # training binary
    epochs=50,
    batch_size=16,
    validation_split=0.2,   # 20% of training data for validation
    verbose=1               # shows training progress and bars you see when running
)

############################################################################

# Evaluate the model on test data
loss, accuracy = mlp_model.evaluate(X_test_scaled, y_test_bin)
print(f"\nMLP Test Accuracy: {accuracy:.4f}")

# Make predictions on the test set
# predict gives probabilities, so flatten them into a 1D array
y_mlp_probs = mlp_model.predict(X_test_scaled).ravel()

# Convert probabilities to binary predictions using a threshold of 0.5
y_mlp_pred = (y_mlp_probs >= 0.5).astype(int)

# Print confusion matrix
print("\nMLP Confusion Matrix:")
print(confusion_matrix(y_test_bin, y_mlp_pred))

# Print precision, recall, f1-score for each class
print("\n MLP Classification Report:")
print(classification_report(y_test_bin, y_mlp_pred))

# Print AUC score to evaluate performance
print("MLP ROC AUC Score:", round(roc_auc_score(y_test_bin, y_mlp_probs), 3))

# comparison chart
def get_metrics(name, y_true, y_pred, y_probs=None):
    return {
        "Model": name,
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred, pos_label=1),
        "Recall": recall_score(y_true, y_pred, pos_label=1),
        "F1 Score": f1_score(y_true, y_pred, pos_label=1),
        "AUC": (
            roc_auc_score(y_test_bin, y_probs)
            if y_probs is not None
            else None)}

# list of results
results = []

# Logistic Regression
results.append(get_metrics("Logistic Regression", y_test, y_pred, model.predict_proba(X_test_scaled)[:, 1]))

# Random Forest with all of the features
results.append(get_metrics("Random Forest", y_test, y_rf_pred, rf_model.predict_proba(X_test_scaled)[:, 1]))

# SVM
results.append(get_metrics("SVM", y_test, y_svm_pred, svm_model.predict_proba(X_test_scaled)[:, 1]))

# gradient boosting
results.append(get_metrics("Gradient Boosting", y_test, y_gb_pred, gb_model.predict_proba(X_test_scaled)[:, 1]))

# random forest with just the top 15 features
results.append(get_metrics("Random Forest (Top 15)", y_test, y_pred_top, y_probs_top))

# MLP Trained on binary labels, predicts if its 0 or 1
y_pred_mlp = (mlp_model.predict(X_test_scaled) > 0.5).astype("int").flatten()
y_probs_mlp = mlp_model.predict(X_test_scaled).flatten()
results.append(get_metrics("MLP", y_test_binary, y_pred_mlp, y_probs_mlp))

# Build leaderboard
leaderboard = pd.DataFrame(results).set_index("Model")
leaderboard = leaderboard.round(4)

# Display
print("\n Model Comparison Leaderboard:")
print(leaderboard)
print(" ")

disp.display(leaderboard)

#compare classification methods bar graph
ax = leaderboard.plot(kind='barh', figsize=(10, 6), title="Model Comparison", legend=True)
plt.xlabel("Score")
ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.tight_layout(rect=[0, 0, 0.85, 1])
plt.show()

# Based on the overall strength of our Random Forest model
# We will move forward with a SHAP test of the misclassified
# patients to determine which features led to their misclassification
# Below is a code block to determine which patients were misclassified

# Convert to a DataFrame for analysis
results_df = pd.DataFrame({
    "Patient Index": range(len(y_test)),
    "True Label": y_test.values,
    "Predicted Label": y_pred_top,
    "Predicted Probability (Malignant)": y_probs_top
})

# Identify misclassifications
misclassified = results_df[results_df["True Label"] != results_df["Predicted Label"]].copy()

# Label the error type
misclassified["Error Type"] = misclassified.apply(
    lambda row: "False Negative" if row["True Label"] == 1 else "False Positive",
    axis=1
)

# Output
import pandas as pd
import IPython.display as disp

disp.display(misclassified)

## Determine which patients were misclassified in the MLP model

import pandas as pd

# Build results DataFrame
mlp_results_df = pd.DataFrame({
    "Index": range(len(y_test)),
    "True Label": y_test.values,
    "Predicted Label": y_pred_mlp,
    "Probability (Malignant)": y_probs_mlp
})

# Identify misclassified patients
mlp_misclassified = mlp_results_df[
    mlp_results_df["True Label"] != mlp_results_df["Predicted Label"]
].copy()

# Add error type column
mlp_misclassified["Error Type"] = mlp_misclassified.apply(
    lambda row: "False Negative" if row["True Label"] == 1 else "False Positive",
    axis=1
)

# Display

disp.display(mlp_misclassified)

# Recreate SHAP explainer using final RF model and RFE-selected features
explainer_rfe = shap.Explainer(
    rf_rfe,
    X_train_rfe_scaled,
    feature_names=selected_features_rfe
)

# Compute SHAP values for test set
shap_values = explainer_rfe(X_test_rfe_scaled)

# Confirm shape
print("SHAP values shape:", shap_values.values.shape)  # should be (114, N_features)

## Finally, we will conduct an RFE (recursive feature elimination) and a random forest model to determine
## if feature elimination results in these three patients being misclassified.

# Scale full feature set
scaler_rfe = StandardScaler()
X_train_scaled_full = scaler_rfe.fit_transform(X_train)
X_test_scaled_full = scaler_rfe.transform(X_test)

# Use RFE to select top features
rf_base = RandomForestClassifier(n_estimators=100, random_state=42)
rfe = RFE(estimator=rf_base, n_features_to_select=8)  # you can adjust this number
rfe.fit(X_train_scaled_full, y_train)

# Extract selected feature names
selected_features_rfe = X_train.columns[rfe.support_].tolist()
print("RFE selected features:", selected_features_rfe)

# Step 4: Subset the original (unscaled) data
X_train_rfe = X_train[selected_features_rfe]
X_test_rfe = X_test[selected_features_rfe]

# Step 5: Rescale the subset for final training
scaler_final = StandardScaler()
X_train_rfe_scaled = scaler_final.fit_transform(X_train_rfe)
X_test_rfe_scaled = scaler_final.transform(X_test_rfe)

# Step 6: Train final Random Forest on RFE-selected features
rf_rfe = RandomForestClassifier(n_estimators=100, random_state=42)
rf_rfe.fit(X_train_rfe_scaled, y_train)

# Step 7: Predict and evaluate
y_pred_rfe = rf_rfe.predict(X_test_rfe_scaled)
y_probs_rfe = rf_rfe.predict_proba(X_test_rfe_scaled)[:, 1]

print("\nC onfusion Matrix (RFE RF):")
print(confusion_matrix(y_test, y_pred_rfe))

print("\n Classification Report (RFE RF):")
print(classification_report(y_test, y_pred_rfe))

print("AUC Score (RFE RF):", round(roc_auc_score(y_test, y_probs_rfe), 4))

# Create results DataFrame for rf_rfe model
rfe_results_df = pd.DataFrame({
    "Index": range(len(y_test)),
    "True Label": y_test.values,
    "Predicted Label": y_pred_rfe,
    "Probability (Malignant)": y_probs_rfe
})

# Filter for misclassified cases
rfe_misclassified = rfe_results_df[
    rfe_results_df["True Label"] != rfe_results_df["Predicted Label"]
].copy()

# Label the error type
rfe_misclassified["Error Type"] = rfe_misclassified.apply(
    lambda row: "False Negative" if row["True Label"] == 1 else "False Positive",
    axis=1
)

# Display the results
disp.display(rfe_misclassified)

import shap

misclassified_indices = [16, 24, 42]

for i in misclassified_indices:
    print(f"\nWaterfall plot for misclassified patient {i} — False Negative")

    explanation = shap.Explanation(
        values=shap_values.values[i, :, 1],              # SHAP values for class 1
        base_values=shap_values.base_values[i, 1],       # base value for class 1
        data=X_test_top_scaled[i],
        feature_names=top_features
    )

    shap.plots.waterfall(explanation)

count = 0

for i in range(len(y_test)):
    if y_pred_top[i] == y_test.values[i]:  # correctly classified
        print(f"\nSHAP waterfall for correctly classified patient {i}")
        print(f"True Label: {y_test.values[i]} | Predicted Label: {y_pred_top[i]} | Prob (malignant): {round(y_probs_top[i], 2)}")

        explanation = shap.Explanation(
            values=shap_values.values[i, :, 1],
            base_values=shap_values.base_values[i, 1],
            data=X_test_top_scaled[i],
            feature_names=top_features
        )

        shap.plots.waterfall(explanation)

        count += 1
        if count == 3:
            break

## We will now adjust the threshold using our updated feature set to determine
## the threshold which nets zero false negatives

# Adjust threshold
rfe_threshold = 0.13
y_pred_rfe_adjusted = (y_probs_rfe >= rfe_threshold).astype(int)

# Re-evaluate
print(f"\n Confusion Matrix (RFE RF @ Threshold = {rfe_threshold}):")
print(confusion_matrix(y_test, y_pred_rfe_adjusted))

print(f"\n Classification Report (RFE RF @ Threshold = {rfe_threshold}):")
print(classification_report(y_test, y_pred_rfe_adjusted))

print(" AUC Score (unchanged):", round(roc_auc_score(y_test, y_probs_rfe), 4))

# Create results DataFrame using adjusted predictions
rfe_adjusted_results_df = pd.DataFrame({
    "Index": range(len(y_test)),
    "True Label": y_test.values,
    "Predicted Label": y_pred_rfe_adjusted,
    "Probability (Malignant)": y_probs_rfe
})

# Filter for misclassified cases
rfe_adjusted_misclassified = rfe_adjusted_results_df[
    rfe_adjusted_results_df["True Label"] != rfe_adjusted_results_df["Predicted Label"]
].copy()

# Label the error type
rfe_adjusted_misclassified["Error Type"] = rfe_adjusted_misclassified.apply(
    lambda row: "False Negative" if row["True Label"] == 1 else "False Positive",
    axis=1
)

# Display the results
disp.display(rfe_adjusted_misclassified)